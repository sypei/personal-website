<html>
	<head>
		<title>Siyou Pei - Expressive AR/VR sensing and interaction</title>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta name="description" content="Siyou Pei, AR/VR enthusiast, HCI researcher, Ph.D. student at HiLab@UCLA ECE with Prof Yang Zhang">


		<meta property="og:title" content="Siyou Pei - Expressive AR/VR sensing and interaction">
		<meta property="og:image" content="https://www.sypei.com/img/Siyou_table3.png">
		<meta property="og:url" content="https://www.sypei.com">
		<meta property="og:description" content="Siyou Pei, AR/VR enthusiast, HCI researcher, Ph.D. student at HiLab@UCLA ECE with Prof Yang Zhang">
		<!-- <meta property='og:image:width' content='1200' /> -->
		<!-- <meta property='og:image:height' content='627' /> -->



		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
		<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.7/css/all.min.css"> -->
		<style type="text/css">
			body {
				min-height: 100vh;
				background-color: papayawhip; 
				margin: 0 auto;
			}
			.fa {
				  padding: 0rem;
				  font-size: 2rem;
				  width: 3rem;
				  text-align: left;
				  text-decoration: none;
				  border-radius: 50%;
				}
			.social{
				width: 6rem;
				font-size: 4rem;
			}
			body {
				  font-family: Josefin Sans, serif  !important;
				}
			h3,h4,.h3,.h4 {
			  font-family: Josefin Sans, serif  !important;
			  font-weight: bold;
				}
		.strong {
		            color: #F2A900;
		            font-weight: bold;
		        }
		.mini-header, .title_name, .email, .fa {
					color:#2774AE; /*ucla blue*/
				}
		.navigator, .navigator a,.fa-trophy{
					color: #FFD100; /*ucla yellow*/
		}
		</style>
		<link href="https://fonts.googleapis.com/css?family=Josefin+Sans:wght@700" rel="stylesheet">

		<link href="css/bootstrap.min.css" rel="stylesheet">
		<link href="css/style.css" rel="stylesheet">
        <link rel="shortcut icon" href="img/ucla.ico" type="image/x-icon">
        <link rel="icon" href="img/ucla.ico" type="image/x-icon">
        <script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		
		  ga('create', 'G-35Q3FR2CS3', 'auto');
		  ga('send', 'pageview');

		</script>

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-35Q3FR2CS3"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-35Q3FR2CS3');
		</script>
	</head>
	<body>
		
		
		<div class="container-fluid">
			
			<div class="row">
				<BR><BR><BR>
			</div>

		 	<div class="row">
				<div  class="col-xs-12 col-sm-6 col-md-5 col-md-offset-2 col-lg-5 col-lg-offset-2 line">
					<BR><BR>
						<a class="title_name" href="index.html" style="font-size: 6rem;font-family: 'Josefin Sans'">Siyou Pei</a>				
						
						&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><br>
						<a href="https://twitter.com/SiyouPei" class="fa fa-twitter social" target="_blank" title="twitter"></a>
						<a href="https://github.com/sypei"  class="fa fa-github social" target="_blank" title="github"></a>
						<a href="https://scholar.google.be/citations?user=WZd4DYAAAAAJ&hl=en" class="fa fa-graduation-cap social" target="_blank" title = "google scholar"></a>&nbsp;&nbsp;
						<a href="research/Siyou_Pei_CV.pdf" class="fa fa-book social" target="_blank" title="CV"></a>
						
						
						<!-- <a class='email' style="font-size: 2rem;font-family: 'Josefin Sans'"><b>sypei @ ucla dot edu</b></a> -->

						<!-- <span class = "navigator" style="font-size: 2rem;font-family: 'Josefin Sans';">
							<a href="aboutme.html">About Me</a> - 
							<a href="research/Siyou_Pei_CV.pdf" target="_blank">CV</a> - 
							<a href="https://hilab.dev">HiLab</a>
						</span> -->
						<br>
						<br>

			      		<P>
			      		I'm a fifth year Ph.D. candidate, an AR/VR enthusiast, an HCI researcher @<a href="https://hilab.dev" style="text-decoration: underline">HiLab</a>, 
						<a href="https://www.ee.ucla.edu/" style="text-decoration: underline">Department of Electrical & Computer Engineering</a>, 
			      		<a href="https://www.ucla.edu/" style="text-decoration: underline">UCLA</a>, advised by <a href="https://www.yangzhang.dev/" style="text-decoration: underline">Prof. Yang Zhang</a>. 
						I also had the fortune to work with mentors Alex Olwal, David Kim, and Ruofei Du. I will be interning at JPMC AR/VR Research Department this summer.  		
			      		<br>
			      		My research is about Extended Reality interaction with a focus on human body embodiment. 
						Leveraging human expertise of their own body, I morph human body into virtual and physical objects to help users control objects intuitively.
						I also apply anthropomorphism to objects, utilizing common sense to help users easily understand smart environments.
			        	</P>

						<p><b>I am on job market, open to opportunities in both academia and industry.</b></p>
				  </div>
		      		<div class="col-xs-12 col-sm-5 col-md-3 col-lg-3 line">		      		


						<img src="img/Siyou_table3.png"  class="img-responsive">
					</div>
					
			</div>

		</div>
		
		<div class="container-fluid">
			<div class="row">
				
				
			  <div class="col-xs-12 col-sm-6 col-md-5 col-md-offset-2 col-lg-5 col-lg-offset-2 line">
			  		<BR>
				  	<div class="row">
							<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12 pubbody"> 
							<span class="mini-header" id="projects" style="font-family: 'Josefin Sans'">
								Research</span>
							</div>
					</div>
					<BR>
					<div class="research-projects">
                        
                        
                        <div class="row research-project" data-sort="2023-10-22">
		                    <br style="line-height: 1rem">
                            <div class="col-md-5">
		                        <video loop muted playsinline autoplay  poster="research/UIMobilityXR/UIMobilityXR.png" class="img-responsive"  width="600">
                                    <source type="video/mp4" src="research/UIMobilityXR/thumbnail.mp4">
		                       </video> 
							   
		                    </div>
		                    <div class="col-md-7">
		                        <h4>
									UI Mobility Control in XR: Switching UI Positionings between Static, Dynamic, and Self Entities
		                        </h4>
		                        <p class="text-muted">
		                            Siyou Pei, David Kim, Alex Olwal, Yang Zhang, and Ruofei Du (CHI 2024) <br />
		                        </p>
		                        
		                        <p>
									Extended reality (XR) has the potential for seamless user interface (UI) transitions across people, objects, and environments, but UI mobility remains an often-overlooked feature. UI mobility refers to changing a UI's type between static, dynamic, and self UI, based on users' in-situ needs. In response, we facilitated UI mobility between entities with a probing interaction technique Finger Switches and users found it userful in our studies.
		                        </p>
                                <a href="https://dl.acm.org/doi/10.1145/3613904.3642220" target="_blank" title="paper" class="fa fa-book" aria-hidden="true"></a>
		                      
		                        <a href="https://youtu.be/bhsrxvT38_M" target="_blank" title="preview" class="fa fa-youtube-play" aria-hidden="true"></a>

		                         <a href="https://youtu.be/57pLICbQHgQ" target="_blank" title="presentation" class="fa fa-microphone" aria-hidden="true"></a> 
		                    </div>
							
							<div class="col-md-12"> 
								<hr class="dash">
							</div>
		                </div>
                        
                        
                        <div class="row research-project" data-sort="2023-10-22">
		                    <br style="line-height: 1rem">
                            <div class="col-md-5">
		                        <video loop muted playsinline autoplay  poster="research/WheelPose/WheelPose.png" class="img-responsive"  width="600">
                                    <source type="video/mp4" src="research/WheelPose/thumbnail.mp4">
		                       </video> 
							   
		                    </div>
		                    <div class="col-md-7">
		                        <h4>
									WheelPose: Data Synthesis Techniques to Improve Pose Estimation Performance on Wheelchair Users
		                        </h4>
		                        <p class="text-muted">
		                            William Huang, Sam Ghahremani, Siyou Pei, and Yang Zhang (CHI 2024) <br />
		                        </p>
		                        
		                        <p>
									Existing pose estimation models perform poorly on wheelchair users due to a lack of representation in training data. We present a data synthesis pipeline to address this disparity in data collection and subsequently improve pose estimation performance for wheelchair users. Our configurable pipeline generates synthetic data of wheelchair users using motion capture data and motion generation outputs simulated in the Unity game engine, validated by real users and performance evaluation.
		                        </p>
                                <a href="https://dl.acm.org/doi/10.1145/3613904.3642555" target="_blank" title="paper" class="fa fa-book" aria-hidden="true"></a>
		                      
		                        <a href="https://www.youtube.com/watch?v=KWVG5bVYwd4" target="_blank" title="preview" class="fa fa-youtube-play" aria-hidden="true"></a>
                                
                                <a href="https://github.com/hilab-open-source/wheelpose" target="_blank" title="code" class="fa fa-github" aria-hidden="true"></a>

<!--		                         <a href="https://youtu.be/57pLICbQHgQ" target="_blank" title="presentation" class="fa fa-microphone" aria-hidden="true"></a> -->
		                    </div>
							
							<div class="col-md-12"> 
								<hr class="dash">
							</div>
		                </div>
                        
                        
                        <div class="row research-project" data-sort="2023-10-22">
		                    <br style="line-height: 1rem">
                            <div class="col-md-5">
<!--      <img src="research/EmbodiedExploration/EmbodiedExploration.png" class="img-responsive"  width="600">-->
		                        <video loop muted playsinline autoplay  poster="research/EmbodiedExploration/EmbodiedExploration.png" class="img-responsive"  width="600">
                                    <source type="video/mp4" src="research/EmbodiedExploration/thumbnail.mp4">
								   <source type="video/webm" src="research/EmbodiedExploration/thumbnail.webm">
		                       </video> 
							   
		                    </div>
		                    <div class="col-md-7">
		                        <h4>
									Embodied Exploration: Facilitating Remote Accessibility Assessment for Wheelchair Users with Virtual Reality
		                        </h4>
		                        <p class="text-muted">
		                            Siyou Pei, Alexander Chen, Chen Chen, Franklin Mingzhe Li, Megan Fozzard, Hao-yun Chi, Nadir Weibel, Patrick Carrington, Yang Zhang (ASSETS 2023) <br />
		                        </p>
<!--
		                        <p class="strong">
		                            <i class="fa fa-trophy"></i>Best Demo Honorable Mention
		                        </p>
-->
		                        
		                        <p>
									Embodied Exploration is Virtual Reality technique for wheelchair users to evalute accessibility remotely. It delivers the experience of a physical visit while keeping the convenience of remote assessment. We validated the efficacy of Embodied Exploration against photo galleries and virtual tours through user studies. Furthermore, we presented key findings on user perception and usability, leading to design guidelines for future accessibility assessment tools.
		                        </p>
                                <a href="https://doi.org/10.1145/3597638.3608410" target="_blank" title="paper" class="fa fa-book" aria-hidden="true"></a>
								<a href="https://youtu.be/IWj8Lc4u3-E" target="_blank" title="preview" class="fa fa-youtube-play" aria-hidden="true"></a>
		                        <a href="https://www.youtube.com/watch?v=XXgwiLo-TqM" target="_blank" title="presentation" class="fa fa-microphone" aria-hidden="true"></a>
<!--
		                        <a href="https://github.com/forcesight/ForceSight" target="_blank" title="code" class="fa fa-github" aria-hidden="true"></a>
		                        <a href="https://www.youtube.com/watch?v=c7U0BBB5QWU" target="_blank" title="preview" class="fa fa-youtube-play" aria-hidden="true"></a>
-->

		                        <!-- <a href="https://www.youtube.com/watch?v=LBIpIioWTaw" target="_blank" title="presentation" class="fa fa-microphone" aria-hidden="true"></a> -->
		                    </div>
							
							<div class="col-md-12"> 
								<hr class="dash">
							</div>
		                </div>
					
						<div class="row research-project" data-sort="2022-08-07">
		                    
		                    <br style="line-height: 1rem">
                            <div class="col-md-5">
		                        <video loop muted playsinline autoplay   poster="research/ForceSight/ForceSight.png" class="img-responsive"  width="600">
								   <source type="video/mp4" src="research/ForceSight/thumbnail.mp4">
                                    <source type="video/webm" src="research/ForceSight/thumbnail.webm">
		                       </video> 
							   
		                    </div>
		                    <div class="col-md-7">
		                        <h4>
									ForceSight: Non-Contact Force Sensing with Laser Speckle Imaging
		                        </h4>
		                        <p class="text-muted">
		                            Siyou Pei, Pradyumna Chari, Xue Wang, Xiaoying Yang, Achuta Kadambi, Yang Zhang (UIST 2022) <br />
		                        </p>
		                        <p class="strong">
		                            <i class="fa fa-trophy"></i>Best Demo Honorable Mention
		                        </p>
		                        
		                        <p>
									We present ForceSight, a non-contact force sensing approach using laser speckle imaging. Our key observation is that object surfaces deform in the presence of force. This deformation, though very minute, manifests as observable and discernible laser speckle shifts, which we leverage to sense the applied force. To investigate the feasibility of our approach, we conducted studies on a wide variety of materials. We also demonstrated the applicability with several example applications.  
		                        </p>
		                        <a href="https://doi.org/10.1145/3526113.3545622" title="paper" target="_blank" class="fa fa-book" aria-hidden="true"></a>
		                        
		                        <a href="https://github.com/forcesight/ForceSight" target="_blank" title="code" class="fa fa-github" aria-hidden="true"></a>
		                        <a href="https://www.youtube.com/watch?v=c7U0BBB5QWU" target="_blank" title="preview" class="fa fa-youtube-play" aria-hidden="true"></a>

		                        <!-- <a href="https://www.youtube.com/watch?v=LBIpIioWTaw" target="_blank" title="presentation" class="fa fa-microphone" aria-hidden="true"></a> -->
		                    </div>
							
							<div class="col-md-12"> 
								<hr class="dash">
							</div>
		                </div>
					
		                <div class="row research-project" data-sort="2022-05-02">
		                    
		                    <br style="line-height: 1rem">
                            <div class="col-md-5">
<!--                                <img src="research/HandInterfaces/HandInterfaces.png"  class="img-responsive"  width="600">-->
		                        <video loop muted playsinline autoplay  poster="research/HandInterfaces/HandInterfaces.png" class="img-responsive"  width="600">
                                    <source type="video/mp4" src="research/HandInterfaces/thumbnail.mp4">
								   <source type="video/webm" src="research/HandInterfaces/thumbnail.webm">
		                       </video> 
							   
		                    </div>
		                    <div class="col-md-7">
		                        <h4>
									Hand Interfaces: Using Hands to Imitate Objects in AR/VR for Expressive Interactions
		                        </h4>
		                        <p class="text-muted">
		                            Siyou Pei, Alexander Chen, Jaewook Lee, Yang Zhang (CHI 2022) <br />
		                        </p>
		                        <p class="strong">
		                            <i class="fa fa-trophy"></i>Best Paper Honorable Mention
		                        </p>
		                        
		                        <p>
									A new interaction technique that lets users' hands become virtual objects by imitating the objects themselves. For example, a thumbs-up hand pose is used to mimic a joystick. We created a wide array of interaction designs around this idea to demonstrate its applicability in object retrieval and interactive control tasks. Collectively, we call these interaction designs Hand Interfaces. 
		                        </p>
		                        <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501898" title="paper" target="_blank" class="fa fa-book" aria-hidden="true"></a>
		                        
		                        <a href="https://github.com/handinterfaces/Hand-Interfaces" target="_blank" title="code" class="fa fa-github" aria-hidden="true"></a>
		                        <a href="https://www.youtube.com/watch?v=ATg3M4QsfEQ" target="_blank" title="preview" class="fa fa-youtube-play" aria-hidden="true"></a>

		                        <a href="https://www.youtube.com/watch?v=LBIpIioWTaw" target="_blank" title="presentation" class="fa fa-microphone" aria-hidden="true"></a>
		                    </div>
							
							<div class="col-md-12"> 
								<hr class="dash">
							</div>
		                </div>

		                <div class="row research-project" data-sort="2021-11-01">
		                	<br style="line-height: 1rem">
		                    <div class="col-md-5">
		                       <video loop muted playsinline autoplay  poster="research/AURITUS/AURITUS_Hardware.png" class="img-responsive" width="600">
								   <source type="video/mp4" src="research/AURITUS/thumbnail.mp4">
								   <source type="video/webm" src="research/AURITUS/thumbnail.webm">
		                       </video>
							   
		                    </div>
		                    <div class="col-md-7">
		                        <h4>
									AURITUS: An Open-Source Optimization Toolkit for Training and Development of Human Movement Models and Filters Using Earables
		                        </h4>
		                        <p class="text-muted">
		                            Swapnil Sayan Saha, Mr. Sandeep Singh Sandha, Siyou Pei, Vivek Jain, Mr. Ziqi Wang, Yuchen Li, Ankur Sarker, Prof. Mani Srivastava (IMWUT 2021) <br />
		                        </p>
		                        
		                        <p>
									 AURITUS is an extendable and open-source optimization toolkit designed to enhance and replicate earable applications. AURITUS handles data collection, pre-processing, and labeling tasks using graphical tools and provides a hardware-in-the-loop (HIL) optimizer and TinyML interface to develop lightweight and real-time machine-learning models for activity detection and filters for head-pose tracking.
		                        </p>
		                        <a href="research/AURITUS/imwut21_AURITUS.pdf" title="paper" target="_blank" class="fa fa-book" aria-hidden="true"></a>&nbsp
		                       
		                        <a href="https://github.com/nesl/auritus" target="_blank" title="code" class="fa fa-github" aria-hidden="true"></a>
		                        <!-- <a href="" target="_blank" class="fa fa-youtube-play" aria-hidden="true"></a>&nbsp
		                        <a href="" target="_blank" class="fa fa-microphone" aria-hidden="true"></a> -->
		                    </div>
							
							<div class="col-md-12"> 
								<hr class="dash">
							</div>
		                </div>
							
						<div class="row research-project" data-sort="2021-07-24">
		                    <div class="col-md-5">
		        
		                       <img src="research/QuickQuestion/QuickQuestionRLmodel.png"  class="img-responsive"  width="600">
							   
		                    </div>
		                    <div class="col-md-7">
		                        <h4>
									Quick Question: Interrupting Users for Microtasks with Reinforcement Learning
		                        </h4>
		                        <p class="text-muted">
		                            Bo-Jhang Ho, Bharathan Balaji, Mehmet Koseoglu, Sandeep Sandha, Siyou Pei, Mani Srivastava (ICML 2021 Workshop on Human in the Loop Learning) <br/>
		                        </p>
		                        
		                        <p>
									Human attention is a scarce resource in modern computing. Quick Question explores use of reinforcement learning (RL) to schedule microtasks while minimizing user annoyance and compare its performance with supervised learning. We model the problem as a Markov decision process and use Advantage Actor Critic algorithm to identify interruptible moments.
		                        </p>
		                        <a href="https://www.icml-hill.com/_files/ugd/4ce291_9e413199c4e343459b0596ccb3d09738.pdf" target="_blank" class="fa fa-book" title="paper" aria-hidden="true"></a>&nbsp
		                      
		                        <a href="https://www.icml-hill.com/_files/ugd/4ce291_5f3f2c04924a4a4ba0fbac8b9d51fc78.pdf" target="_blank" class="fa fa-paperclip" title="slides" aria-hidden="true"></a>
		                    </div>
							
							<div class="col-md-12"> 
								<hr class="dash">
							</div>
		                </div>

						
		              
		            </div>
	            	</div>
				
			
				
			<div class="col-xs-12 col-sm-5 col-md-3 col-lg-3 line">
					<br><br><br>

					<a class="twitter-timeline" 
					width="600"
					height="600"
					data-chrome="nofooter noheader noborder transparent"
					data-border-color="#ffffff"
					href="https://twitter.com/SiyouPei" 
					data-widget-id="515768611978223616">
						Tweets by @SiyouPei</a>
					<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
			  
					<BR>
					<span class="mini-header" style="font-family: 'Josefin Sans';">
						<h3>News</h3>
					</span>
                    
                    <P>
						May 2024: Presented <a href="https://dl.acm.org/doi/10.1145/3613904.3642220" target = "_blank">UI Mobility in XR</a> at CHI 2024.
					</P>
                    
                    <P>
						Feb 2024: Reviewed CHI 2024 Late-Breaking Work as Chair Committee.
					</P>
                    <P>
						Oct 2023: Presented <a href="https://doi.org/10.1145/3597638.3608410" target="_blank">Embodied Exploration</a> at ASSETS 2023, New York.
					</P>
                    <P>
						Oct 2023: Reviewed CHI 2024 Papers.
					</P>
                    
					<P>
						July 2023: Successfully organized a non-profit summer program <a href="https://sites.google.com/site/computingcircle/" target="_blank">LACC 2023</a> at UCLA.
					</P>
                    <P>
						May 2023: Reviewed UIST 2023 Papers.
					</P>
					<P>
						Apr 2023: Reviewed ISMAR 2023 Papers.
					</P>
					<P>
						Jan-Apr 2023: Interned at Google in San Francisco.
					</P>
					<P>
						Mar 2023: Reviewed DIS 2023 Papers.
					</P>
					<P>
						Jan 2023: Reviewed CHI 2023 Late-Breaking Work.
					</P>
					<P>
						Sep-Dec 2023: Interned at Google in Los Angeles.
					</P>
					<P>
						Nov 2022: Received UIST Best Demo Honorable Mention Award for <a href="https://doi.org/10.1145/3526113.3545622" target="_blank">ForceSight</a>.
					</P>
					<P>
						Oct 2022: Presented <a href="https://doi.org/10.1145/3526113.3545622" target="_blank">ForceSight</a> at UIST '22, Bend, OR.
					</P>
					<P>
						Oct 2022: Reviewed CHI 2023 Papers.
					</P>
					<P>
						August 2022: Passed the <a href="https://www.ee.ucla.edu/ph-d-student-information/#oralexam" target="_blank">Oral Qualifying Examination</a> at Department of Electrical & Computer Engineering and became a Ph.D candidate.
					</P>
					<P>
						May 2022: Reviewed UIST 2022 Papers.
					</P>
					<P>
						May 2022: Presented <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501898" target="_blank">Hand Interfaces</a> at CHI '22, New Orleans, LA.
					</P>
					<P>
						April 2022: Received CHI Honorable Mention Award for <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501898" target="_blank">Hand Interfaces</a>.
					</P>
					<P>
						April 2022: Reviewed CHI 2022 Late-Breaking Work.
					</P>
					<P>
						Mar 2022: Passed the <a href="https://www.ee.ucla.edu/ph-d-student-information/#preliminaryexam" target="_blank">Preliminary Exam</a> at Department of Electrical & Computer Engineering.
					</P>
					<P>
						Dec 2021: Turned in MS thesis and obtained the MS degree.
					</P>
					<P>
						Nov 2021: Reviewed CHI 2022 Papers.
					</P>
					<P>
						Feb 2021: Reviewed CHI 2021 Late-Breaking Work.
					</P>
					<P>
						Sep 2019: Started MS-Ph.D. program at University of California, Los Angeles.
					</P>			
				
				
					<br>
						
					<img src="img/bruin.png"  class="img-responsive" width=180>
					
			  		<BR>
			  </div>
				
		</div>

		<BR>
		<small>&copy; Siyou Pei, <a href="https://github.com/sypei/personal_website">Source Code</a>.</small>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    	<script src="js/bootstrap.min.js"></script>
	</body>
</html>